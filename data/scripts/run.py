#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç»Ÿä¸€å…¥å£: ç”Ÿæˆå„äº‘å‚å•†/æœåŠ¡å•† IP åˆ—è¡¨ -> æ ¡éªŒæ˜¯å¦ä¸ç°æœ‰ data/idc ä¸‹æ–‡ä»¶å®Œå…¨ä¸€è‡´ -> ç§»åŠ¨è¦†ç›–ã€‚

ä½¿ç”¨æ–¹å¼:
  åŸºæœ¬æ‰§è¡Œ (ä»…åœ¨å…¨éƒ¨ä¸ data/idc åŸºå‡†å®Œå…¨ä¸€è‡´æ—¶æ‰ä¼šè¦†ç›–å¹¶åˆ é™¤ output ä¸­å¯¹åº”æ–‡ä»¶):
  Basic usage (will only overwrite and delete output files if they match the baseline in data/idc):
    python3 run.py

  ç”Ÿæˆå¹¶æ ¡éªŒ, ä¸ä¿®æ”¹ data/idc (ä¸ä¼šå¤åˆ¶/è¦†ç›–/åˆ é™¤):
    Generate and verify without modifying data/idc (no copy/overwrite/delete):
    python3 run.py --verify-only

  å¼ºåˆ¶æ›´æ–°åŸºå‡† (å¿½ç•¥å·®å¼‚, è¦†ç›– data/idc, å¹¶åˆ é™¤ output æºæ–‡ä»¶):
    Force update baseline (ignore differences, overwrite data/idc, and delete output source files):
    python3 run.py --force

  å¼ºåˆ¶æ›´æ–°ä½†ä¿ç•™ output ç›®å½•ä¸­çš„ç”Ÿæˆæ–‡ä»¶ (ç”¨äºæ’æŸ¥/æ¯”å¯¹):
    Force update but keep generated files in output directory (for debugging/diffing):
    python3 run.py --force --keep-output

  ä»…ç”Ÿæˆå¹¶æ ¡éªŒ, åŒæ—¶ä¿ç•™ output æ–‡ä»¶ (è°ƒè¯•å·®å¼‚è¿‡ç¨‹):
    Generate and verify only, keeping output files (for debugging differences):
    python3 run.py --verify-only --keep-output

å‚æ•°è¯´æ˜:
  --verify-only   åªç”Ÿæˆä¸æ¯”è¾ƒ, ä¸å†™å…¥ idc ç›®å½• (ä¸äº§ç”Ÿå‰¯ä½œç”¨)
                  Only generate and compare, do not write to idc directory (no side effects)
  --force         å³ä¾¿å­˜åœ¨å·®å¼‚ä¹Ÿè¦†ç›– idc ç›®å½• (ä¸åŸºå‡†æ›´æ–°æ—¥å¸¸æ“ä½œæ—¶ä½¿ç”¨, éœ€äººå·¥ç¡®è®¤)
                  Force overwrite idc directory even if differences exist (used for baseline updates, requires manual confirmation)
  --keep-output   æ‰§è¡Œåä¿ç•™ output ä¸‹æ‰€æœ‰ç”Ÿæˆæ–‡ä»¶ (é»˜è®¤ä¼šåœ¨ç§»åŠ¨ååˆ é™¤å®ƒä»¬)
                  Keep all generated files in output directory after execution (default deletes them after moving)

å¤„ç†é€»è¾‘æ¦‚è¦:
  1. è¿è¡Œå­è„šæœ¬æ‹‰å–å„äº‘å‚å•†æœ€æ–° CIDR -> å†™å…¥ data/scripts/output/*.txt
  2. ç›´æ¥ä¸‹è½½å¹¶å¤„ç†é¢å¤–ä¾›åº”å•† (akamai / apple / linode / zscaler)
  3. ä¸ data/idc ä¸‹åŒåæ–‡ä»¶é€ä¸€åšå†…å®¹(é€è¡Œ)ä¸¥æ ¼æ¯”è¾ƒ
  4. é»˜è®¤è‹¥å­˜åœ¨ä»»ä½•å·®å¼‚åˆ™ç»ˆæ­¢å¹¶è¿”å›é 0 (æ»¡è¶³â€œå¿…é¡»å®Œå…¨ä¸€è‡´â€çš„çº¦æŸ)
  5. åœ¨æ»¡è¶³ä¸€è‡´æˆ–ä½¿ç”¨ --force æƒ…å†µä¸‹å¤åˆ¶è‡³ data/idc (é»˜è®¤å¤åˆ¶ååˆ é™¤ output æºæ–‡ä»¶; ä¼  --keep-output åˆ™ä¿ç•™)

é€€å‡ºç :
Exit codes:
  0  æˆåŠŸ (å…¨éƒ¨ä¸€è‡´ æˆ– ä½¿ç”¨ --force å·²è¦†ç›–)
     Success (all files match or --force has been used to overwrite)
  1  å­˜åœ¨å·®å¼‚ä¸”æœªæŒ‡å®š --force
     Differences found and --force not specified
  2  å­è„šæœ¬æˆ–ä¸‹è½½ç­‰è¿‡ç¨‹å‘ç”Ÿå¼‚å¸¸
     Exception occurred in sub-scripts or downloads

å»ºè®®å·¥ä½œæµ:
  # æŸ¥çœ‹æ˜¯å¦æœ‰å˜åŒ– (å®‰å…¨)
  python3 run.py --verify-only

  # è‹¥ç¡®è®¤éœ€è¦æ›´æ–°åŸºå‡† (æœ‰å·®å¼‚ä¸”è¯„ä¼°æ— é—®é¢˜)
  python3 run.py --force

  # è‹¥æƒ³ä¿ç•™ä¸­é—´æ–‡ä»¶ä½œè¿›ä¸€æ­¥ diff
  python3 run.py --force --keep-output

é¢å¤–è¯´æ˜:
  ä¸ºæ»¡è¶³â€œç”Ÿæˆçš„æ–‡ä»¶è¦ä¸ç°åœ¨å·²ç»å­˜åœ¨çš„å®Œå…¨ä¸€è‡´â€è¿™ä¸€éœ€æ±‚, é»˜è®¤å‘ç°å·®å¼‚ä¼šç›´æ¥æŠ¥é”™é€€å‡º (code 1)ã€‚
"""
from __future__ import annotations
import subprocess
import hashlib
import filecmp
import shutil
import sys
import json
from pathlib import Path
import argparse
import difflib

SCRIPT_DIR = Path(__file__).parent
OUTPUT_DIR = SCRIPT_DIR / "output"
IDC_DIR = SCRIPT_DIR.parent / "idc"

# éœ€è¦è¿è¡Œçš„å­è„šæœ¬ (ä¿æŒä¸åŸ bulk.py ä¸€è‡´)
PROVIDER_SCRIPTS = [
    "aws/aws.py",
    "azure/azure.py",
    "gcp/get_gcp.py",
    "digitalOcean/do.py",
    "oci/oci.py",
]

# ç”± run.py å†…éƒ¨ç›´æ¥ä¸‹è½½å¤„ç†çš„ä¾›åº”å•† (é€»è¾‘æ¥è‡ª bulk.py)
EXTRA_PROVIDERS = {
    "akamai": {
        "type": "multi-txt",
        "sources": [
            "https://raw.githubusercontent.com/femueller/cloud-ip-ranges/refs/heads/master/akamai-v4-ip-ranges.txt",
            "https://raw.githubusercontent.com/femueller/cloud-ip-ranges/refs/heads/master/akamai-v6-ip-ranges.txt",
        ],
    },
    "apple": {
        "type": "csv-first-column",
        "source": "https://raw.githubusercontent.com/femueller/cloud-ip-ranges/refs/heads/master/apple-icloud-private-relay-ip-ranges.csv",
    },
    "linode": {
        "type": "geofeed",
        "source": "https://raw.githubusercontent.com/femueller/cloud-ip-ranges/refs/heads/master/linode.txt",
    },
    "zscaler": {
        "type": "json-list",
        "source": "https://raw.githubusercontent.com/femueller/cloud-ip-ranges/refs/heads/master/zscaler-cloud-ip-ranges.json",
        "json_key": "hubPrefixes",
    },
}

# æ‰€æœ‰æœ€ç»ˆåº”è¯¥å­˜åœ¨çš„è¾“å‡ºæ–‡ä»¶å (ä¸ data/idc ä¸‹ä¿æŒä¸€è‡´)
EXPECTED_FILES = [
    "aws.txt",
    "azure.txt",
    "gcp.txt",
    "digitalocean.txt",
    "oracle.txt",
    "akamai.txt",
    "apple.txt",
    "linode.txt",
    "zscaler.txt",
]


def run_sub_script(rel_path: str) -> None:
    script = SCRIPT_DIR / rel_path
    if not script.exists():
        print(f"âš ï¸ å­è„šæœ¬ä¸å­˜åœ¨, è·³è¿‡: {rel_path}")
        return
    print(f"ğŸš€ è¿è¡Œå­è„šæœ¬: {rel_path}")
    subprocess.run([sys.executable, str(script)], check=True, cwd=script.parent)


def curl_download(url: str) -> str | None:
    try:
        result = subprocess.run(["curl", "-sL", url], check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
        return result.stdout.decode("utf-8", errors="ignore")
    except subprocess.CalledProcessError as e:
        print(f"âŒ ä¸‹è½½å¤±è´¥: {url} => {e.stderr.decode(errors='ignore')}")
        return None


def save_sorted_unique(lines: set[str], path: Path) -> None:
    with path.open("w", encoding="utf-8") as f:
        for line in sorted(lines):
            f.write(line.strip() + "\n")
    print(f"âœ… å†™å…¥ {len(lines)} æ¡: {path.name}")


def process_extra_providers():
    for name, meta in EXTRA_PROVIDERS.items():
        out_file = OUTPUT_DIR / f"{name}.txt"
        t = meta["type"]
        print(f"ğŸ› ï¸ å¤„ç† {name} ({t})")
        if t == "multi-txt":
            lines: set[str] = set()
            for u in meta["sources"]:
                content = curl_download(u)
                if not content:
                    continue
                for line in content.splitlines():
                    line = line.strip()
                    if line and not line.startswith('#'):
                        lines.add(line)
            save_sorted_unique(lines, out_file)
        elif t in ("csv-first-column", "geofeed"):
            content = curl_download(meta["source"])
            if not content:
                continue
            lines: set[str] = set()
            for line in content.splitlines():
                line = line.strip()
                if not line or line.startswith('#'):
                    continue
                cidr = line.split(',', 1)[0].strip()
                if cidr:
                    lines.add(cidr)
            save_sorted_unique(lines, out_file)
        elif t == "json-list":
            content = curl_download(meta["source"])
            if not content:
                continue
            try:
                data = json.loads(content)
                key = meta["json_key"]
                lines = set(data.get(key, []))
                save_sorted_unique(lines, out_file)
            except json.JSONDecodeError:
                print(f"âŒ JSON è§£æå¤±è´¥: {name}")
        else:
            print(f"âš ï¸ æœªçŸ¥ç±»å‹: {t}")


def sha256sum(path: Path) -> str:
    h = hashlib.sha256()
    with path.open('rb') as f:
        for chunk in iter(lambda: f.read(8192), b''):
            h.update(chunk)
    return h.hexdigest()


def compare_and_report(output_file: Path, idc_file: Path) -> tuple[bool, str | None]:
    if not idc_file.exists():
        return False, f"âŒ ç¼ºå¤±åŸºå‡†æ–‡ä»¶: {idc_file.name}"
    if filecmp.cmp(output_file, idc_file, shallow=False):
        return True, None
    # ç”Ÿæˆå·®å¼‚æ–‡æœ¬ (æˆªå–å‰è‹¥å¹²è¡Œ)
    with output_file.open(encoding='utf-8') as f1, idc_file.open(encoding='utf-8') as f2:
        out_lines = f1.readlines()
        base_lines = f2.readlines()
    diff = difflib.unified_diff(base_lines, out_lines, fromfile=f"idc/{idc_file.name}", tofile=f"output/{output_file.name}")
    collected = []
    for i, line in enumerate(diff):
        if i > 200:  # é™åˆ¶è¾“å‡º
            collected.append('... (diff çœç•¥)')
            break
        collected.append(line.rstrip('\n'))
    return False, "\n".join(collected)


def move_files(force: bool, keep_output: bool) -> list[str]:
    messages = []
    for fname in EXPECTED_FILES:
        src = OUTPUT_DIR / fname
        if not src.exists():
            messages.append(f"âš ï¸ æœªç”Ÿæˆ: {fname}")
            continue
        dst = IDC_DIR / fname
        if dst.exists() and not force:
            # å·²éªŒè¯ä¸€è‡´æ—¶å†ç§»åŠ¨; è¿™é‡Œå‡è®¾ä¸Šä¸€æ­¥å·²åšä¸€è‡´æ€§æ£€æŸ¥
            pass
        if not keep_output:
            shutil.copy2(src, dst)
            try:
                src.unlink()  # ç§»åŠ¨ååˆ é™¤æºæ–‡ä»¶
                messages.append(f"ğŸ“¦ å·²ç§»åŠ¨å¹¶åˆ é™¤æºæ–‡ä»¶: {dst.relative_to(SCRIPT_DIR.parent)}")
            except OSError as e:
                messages.append(f"âš ï¸ å·²å¤åˆ¶ä½†åˆ é™¤æºæ–‡ä»¶å¤±è´¥ {fname}: {e}")
        else:
            shutil.copy2(src, dst)
            messages.append(f"ğŸ“¦ å·²å¤åˆ¶(ä¿ç•™æºæ–‡ä»¶): {dst.relative_to(SCRIPT_DIR.parent)}")
    return messages


def main():
    parser = argparse.ArgumentParser(description="ç»Ÿä¸€ç”Ÿæˆå¹¶æ ¡éªŒäº‘å‚å•† IP åˆ—è¡¨")
    parser.add_argument('--verify-only', action='store_true', help='ä»…ç”Ÿæˆä¸æ ¡éªŒ, ä¸ç§»åŠ¨æ–‡ä»¶')
    parser.add_argument('--force', action='store_true', help='å¿½ç•¥å·®å¼‚å¼ºåˆ¶è¦†ç›– idc æ–‡ä»¶')
    parser.add_argument('--keep-output', action='store_true', help='ä¿ç•™ output ä¸‹æ–‡ä»¶ (ä¸ç§»åŠ¨)')
    args = parser.parse_args()

    OUTPUT_DIR.mkdir(exist_ok=True)
    IDC_DIR.mkdir(exist_ok=True)

    # 1. è¿è¡Œå­è„šæœ¬ (è¿™äº›è„šæœ¬è‡ªå·±å†™å…¥ OUTPUT_DIR)
    for rel in PROVIDER_SCRIPTS:
        try:
            run_sub_script(rel)
        except subprocess.CalledProcessError as e:
            print(f"âŒ å­è„šæœ¬æ‰§è¡Œå¤±è´¥: {rel} => {e}")
            sys.exit(2)

    # 2. å¤„ç†é¢å¤–ä¾›åº”å•†
    process_extra_providers()

    # 3. æ ¡éªŒæ˜¯å¦éƒ½ç”Ÿæˆ
    missing = [f for f in EXPECTED_FILES if not (OUTPUT_DIR / f).exists()]
    if missing:
        print(f"âš ï¸ ç¼ºå¤±ç”Ÿæˆæ–‡ä»¶: {missing}")

    # 4. ä¸€è‡´æ€§æ£€æŸ¥
    all_ok = True
    diff_reports: list[str] = []
    for fname in EXPECTED_FILES:
        out_file = OUTPUT_DIR / fname
        idc_file = IDC_DIR / fname
        if not out_file.exists() or not idc_file.exists():
            if not idc_file.exists():
                print(f"âš ï¸ åŸºå‡†ä¸å­˜åœ¨(é¦–æ¬¡ç”Ÿæˆ?): {fname}")
            continue
        same, diff_text = compare_and_report(out_file, idc_file)
        if same:
            print(f"âœ… ä¸€è‡´: {fname} ({sha256sum(out_file)})")
        else:
            all_ok = False
            print(f"âŒ å·®å¼‚: {fname}")
            if diff_text:
                diff_reports.append(diff_text)

    if not all_ok and not args.force:
        print("\n================ å·®å¼‚è¯¦æƒ…(æˆªæ–­) ================")
        for d in diff_reports:
            print(d)
            print('----------------------------------------')
        print("å‘ç°å·®å¼‚, å·²æŒ‰è¦æ±‚é˜»æ­¢è¦†ç›– (ä½¿ç”¨ --force å¯å¼ºåˆ¶è¦†ç›–)ã€‚")
        sys.exit(1)

    # 5. ç§»åŠ¨/å¤åˆ¶åˆ° idc ç›®å½• (ä»…åœ¨é verify-only ä¸”æœª keep-output æƒ…å†µä¸‹)
    if not args.verify_only:
        msgs = move_files(force=args.force, keep_output=args.keep_output)
        for m in msgs:
            print(m)
    else:
        print("ğŸ” å·²å®Œæˆæ ¡éªŒ (--verify-only æ¨¡å¼, æœªç§»åŠ¨æ–‡ä»¶)")

    if all_ok or args.force:
        print("\nğŸ‰ å®Œæˆ: æ‰€æœ‰ç”Ÿæˆæ–‡ä»¶ä¸ç°æœ‰å®Œå…¨ä¸€è‡´" if all_ok else "âš ï¸ å·²å¼ºåˆ¶è¦†ç›–å·®å¼‚æ–‡ä»¶")
    sys.exit(0 if all_ok or args.force else 1)


if __name__ == '__main__':
    main()
